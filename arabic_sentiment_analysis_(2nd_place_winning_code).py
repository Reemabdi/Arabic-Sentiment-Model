# -*- coding: utf-8 -*-
"""Arabic Sentiment Analysis (2nd place Winning code)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/reemabdi/arabic-sentiment-analysis-2nd-place-winning-code.1f5f4322-5336-42a4-89ab-f3f0094c87fb.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250804/auto/storage/goog4_request%26X-Goog-Date%3D20250804T022615Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D9ae71c51e437ce8bcfa4fdf0ddc2a3df30f3fd3f86a872f230dffc85d7cb5d243ae6d94296b3f033f59cd05e9fa12a9730a82650693aa4dcd75a7bcf26530e50ebe0ca5130532c0ba1f022db6690f14317d3428f85beb63ef317f587431238a96c35980d44c9297076fab82729a6c520f9cdbaaed8c8f4044c7583878170c5ee49b4e4e60159570cbfd786db8ccd7b653d7a9badd068e17d8e0feaca99e3777a06648591cb9e38eeca1764d3125e45764bbb08a3607ca938536e28c5d28d1ec996c7548bd0bae10e8af35845ca33c5fb59d14b53ac16e590e88623cda96dbed3561110bab67f52b4946c951801c7c0cf4d41ba8bd8823a73a39cc995ac646052
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

from google.colab import files

uploaded = files.upload()

import os
os.listdir()

import zipfile

zip_file = "archive (3).zip"  # name must match exactly

with zipfile.ZipFile(zip_file, 'r') as zip_ref:
    zip_ref.extractall("/content/dataset")

print("✅ Dataset extracted successfully!")

import os

data_path = "/content/dataset"
os.listdir(data_path)

!pip install kaggle

import pandas as pd

# Correct file paths
train_neg = pd.read_csv("/content/dataset/train_Arabic_tweets_negative_20190413.tsv",
                        sep="\t", names=["tweet_id", "tweet_text"])
train_neg["sentiment"] = -1

train_pos = pd.read_csv("/content/dataset/train_Arabic_tweets_positive_20190413.tsv",
                        sep="\t", names=["tweet_id", "tweet_text"])
train_pos["sentiment"] = 1

# Combine data
train_df = pd.concat([train_neg, train_pos], ignore_index=True)
print(train_df.head(), "\nTotal samples:", len(train_df))

import pandas as pd
import os

# Define possible dataset paths
path_neg = "data/train_Arabic_tweets_negative_20190413.tsv"
path_pos = "data/train_Arabic_tweets_positive_20190413.tsv"

# Fallback paths (for Google Colab)
if not os.path.exists(path_neg):
    path_neg = "/content/dataset/train_Arabic_tweets_negative_20190413.tsv"
if not os.path.exists(path_pos):
    path_pos = "/content/dataset/train_Arabic_tweets_positive_20190413.tsv"

# Load datasets
train_neg = pd.read_csv(path_neg, sep="\t", names=["tweet_id", "tweet_text"], encoding="utf-8")
train_neg["sentiment"] = -1

train_pos = pd.read_csv(path_pos, sep="\t", names=["tweet_id", "tweet_text"], encoding="utf-8")
train_pos["sentiment"] = 1

# Combine datasets
train_df = pd.concat([train_neg, train_pos], ignore_index=True)

# Display results
print(train_df.head(), "\nTotal samples:", len(train_df))

import re

def clean_tweet(text):
    text = re.sub(r"http\S+|www\S+|https\S+", '', text)  # remove URLs
    text = re.sub(r'@\w+|#\w+', '', text)                # remove mentions & hashtags
    text = re.sub(r'[^\u0600-\u06FF\s]', '', text)       # keep only Arabic letters
    text = re.sub(r'\s+', ' ', text).strip()             # remove extra spaces
    return text

train_df["clean_text"] = train_df["tweet_text"].apply(clean_tweet)
print(train_df.head())

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer

X = train_df["clean_text"]
y = train_df["sentiment"]

vectorizer = CountVectorizer(max_features=5000)
X_vectorized = vectorizer.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)

from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)
print("Accuracy:", model.score(X_test, y_test))

sample_text = ["انا سعيد جدا اليوم", "انا حزين ومتضايق"]
sample_vectorized = vectorizer.transform(sample_text)
print(model.predict(sample_vectorized))

from sklearn.naive_bayes import MultinomialNB
model = MultinomialNB()
model.fit(X_train, y_train)
print("Accuracy:", model.score(X_test, y_test))

import re

def clean_arabic(text):
    # Remove URLs
    text = re.sub(r"http\S+|www.\S+", "", text)
    # Remove mentions and hashtags
    text = re.sub(r"@\w+|#\w+", "", text)
    # Remove numbers and punctuation
    text = re.sub(r"[^ء-ي\s]", "", text)
    # Remove extra spaces
    text = re.sub(r"\s+", " ", text).strip()
    return text

train_df["tweet_text"] = train_df["tweet_text"].apply(clean_arabic)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))
X = vectorizer.fit_transform(train_df["tweet_text"])
y = train_df["sentiment"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.linear_model import LogisticRegression

model = LogisticRegression(max_iter=1000, solver='lbfgs')
model.fit(X_train, y_train)

print("Improved Accuracy:", model.score(X_test, y_test))

sample_text = ["انا حزين ومتضايق", "انا سعيد جدا اليوم"]
sample_vectorized = vectorizer.transform(sample_text)
print("Predictions:", model.predict(sample_vectorized))

model = LogisticRegression(max_iter=1000, solver='lbfgs', class_weight='balanced')
model.fit(X_train, y_train)
print("Improved Accuracy:", model.score(X_test, y_test))

vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,3))

arabic_stopwords = ["و", "في", "على", "من", "إلى", "عن", "أن", "كان", "ما", "لم", "لا", "هذا", "هذه", "هناك"]

def remove_stopwords(text):
    return " ".join([word for word in text.split() if word not in arabic_stopwords])

train_df["tweet_text"] = train_df["tweet_text"].apply(remove_stopwords)

vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(3,5), max_features=20000)
X = vectorizer.fit_transform(train_df["tweet_text"])

from sklearn.pipeline import FeatureUnion

word_vectorizer = TfidfVectorizer(ngram_range=(1,3), max_features=10000)
char_vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(3,5), max_features=10000)

combined = FeatureUnion([('word', word_vectorizer), ('char', char_vectorizer)])
X = combined.fit_transform(train_df["tweet_text"])

model = LogisticRegression(max_iter=1000, solver='lbfgs', class_weight='balanced', C=5)

from sklearn.model_selection import GridSearchCV

params = {
    'C': [0.1, 1, 5],
    'solver': ['lbfgs', 'liblinear']
}

grid = GridSearchCV(LogisticRegression(max_iter=1000, class_weight='balanced'), params, cv=3)
grid.fit(X_train, y_train)
print("Best Parameters:", grid.best_params_)
print("Best Accuracy:", grid.best_score_)

from sklearn.pipeline import FeatureUnion
from sklearn.feature_extraction.text import TfidfVectorizer

word_vectorizer = TfidfVectorizer(ngram_range=(1,3), max_features=10000)
char_vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(3,5), max_features=10000)

vectorizer = FeatureUnion([("word", word_vectorizer), ("char", char_vectorizer)])
X = vectorizer.fit_transform(train_df["tweet_text"])

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, train_df["sentiment"], test_size=0.2, random_state=42)

model = LogisticRegression(max_iter=1000, solver='liblinear', class_weight='balanced', C=5)
model.fit(X_train, y_train)
print("Improved Accuracy:", model.score(X_test, y_test))

def normalize_arabic(text):
    text = re.sub("[إأآا]", "ا", text)
    text = re.sub("ى", "ي", text)
    text = re.sub("ؤ", "و", text)
    text = re.sub("ئ", "ي", text)
    text = re.sub("ة", "ه", text)
    return text

train_df["tweet_text"] = train_df["tweet_text"].apply(normalize_arabic)

char_vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2,6), max_features=20000)

model = LogisticRegression(max_iter=1000, solver='liblinear', class_weight='balanced', C=10)

from sklearn.ensemble import VotingClassifier
from sklearn.svm import LinearSVC

clf1 = LogisticRegression(max_iter=1000, solver='liblinear', class_weight='balanced', C=10)
clf2 = LinearSVC(class_weight='balanced', C=1)

ensemble = VotingClassifier(estimators=[('lr', clf1), ('svc', clf2)], voting='hard')
ensemble.fit(X_train, y_train)
print("Ensemble Accuracy:", ensemble.score(X_test, y_test))

from sklearn.pipeline import FeatureUnion

word_vectorizer = TfidfVectorizer(ngram_range=(1,3), max_features=10000)
char_vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2,6), max_features=20000)

vectorizer = FeatureUnion([
    ("word", word_vectorizer),
    ("char", char_vectorizer)
])

X = vectorizer.fit_transform(train_df["tweet_text"])

clf1 = LogisticRegression(max_iter=1000, solver='liblinear', class_weight='balanced', C=20)

ensemble = VotingClassifier(
    estimators=[('lr', clf1), ('svc', clf2)],
    voting='soft',
    weights=[2,1]
)

train_df["tweet_text"] = train_df["tweet_text"].apply(remove_stopwords)

ensemble = VotingClassifier(
    estimators=[('lr', clf1), ('svc', clf2)],
    voting='hard',
    weights=[2, 1]
)

ensemble.fit(X_train, y_train)

accuracy = ensemble.score(X_test, y_test)
print("Final Optimized Accuracy:", accuracy)

# ✅ Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, train_df["sentiment"], test_size=0.2, random_state=42)

# ✅ Train Ensemble
ensemble.fit(X_train, y_train)

# ✅ Accuracy
accuracy = ensemble.score(X_test, y_test)
print("Final Optimized Accuracy:", accuracy)

# ✅ Predict on new tweets (using the SAME vectorizer trained on train_df)
sample_tweets = ["التجربة سيئة ولن أكررها", "هذا المنتج رائع جدا"]
X_new = vectorizer.transform(sample_tweets)
predictions = ensemble.predict(X_new)

for tweet, sentiment in zip(sample_tweets, predictions):
    print(f"Tweet: {tweet} → Sentiment: {sentiment}")

X_train, X_test, y_train, y_test = train_test_split(X, train_df["sentiment"], test_size=0.2, random_state=42)

ensemble.fit(X_train, y_train)

accuracy = ensemble.score(X_test, y_test)
print("Final Optimized Accuracy:", accuracy)

sample_tweets = ["التجربة سيئة ولن أكررها", "هذا المنتج رائع جدا"]
X_new = vectorizer.transform(sample_tweets)
predictions = ensemble.predict(X_new)

import joblib
joblib.dump(ensemble, "sentiment_model.pkl")
joblib.dump(vectorizer, "vectorizer.pkl")

pip install streamlit

import streamlit as st
import joblib

model = joblib.load("sentiment_model.pkl")
vectorizer = joblib.load("vectorizer.pkl")

st.title("🔹 Arabic Sentiment Analysis")
user_input = st.text_area("أدخل النص هنا:")
if st.button("تحليل"):
    X_input = vectorizer.transform([user_input])
    result = model.predict(X_input)[0]
    st.write("🔹 النتيجة:", "إيجابي ✅" if result == 1 else "سلبي ❌")

!pip install pyngrok
!ngrok config add-authtoken "30oBRclzllz5rjXjlVbiUwFc11e_2b9GmQUq2Wx9htcwtEEji"

from pyngrok import ngrok
# افتح نفق (Tunnel) على المنفذ 8501
public_url = ngrok.connect(8501)
print("🌐 رابط التطبيق:", public_url)

# تشغيل Streamlit في الخلفية
!streamlit run /content/app.py --server.port 8501 --server.headless true &

!pip install transformers torch arabert

from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

# تحميل Tokenizer و Model
model_name = "aubmindlab/bert-base-arabertv02-twitter"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(
    "CAMeL-Lab/bert-base-arabic-camelbert-da-sentiment"
)

# إنشاء الـ Pipeline
sentiment_pipeline = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)

def analyze_sentiment(text):
    result = sentiment_pipeline(text)[0]
    label = result['label']
    score = result['score']

    # تحويل النتائج إلى عربي
    if "positive" in label.lower():
        return f"✅ النتيجة: إيجابي ({score:.2f})"
    elif "negative" in label.lower():
        return f"❌ النتيجة: سلبي ({score:.2f})"
    else:
        return f"😐 النتيجة: محايد ({score:.2f})"

import streamlit as st

st.title("Arabic Sentiment Analysis (AraBERT)")

text_input = st.text_area("أدخل النص هنا:")

if st.button("تحليل"):
    if text_input.strip() != "":
        st.write(analyze_sentiment(text_input))
    else:
        st.write("⚠️ الرجاء إدخال نص أولاً.")

!pip install pyngrok
from pyngrok import ngrok

# فتح نفق (tunnel) جديد
public_url = ngrok.connect(8501)
print("رابط التطبيق:", public_url)

# تشغيل التطبيق
!streamlit run /content/app.py --server.port 8501 --server.headless true &

label_map = {
    "LABEL_0": "إيجابي",  # or سلبي, based on test
    "LABEL_1": "سلبي"
}

from transformers import pipeline

# Load pipeline
sentiment_pipeline = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)

# Test sample text
text = "الخدمة سيئة للغاية"
result = sentiment_pipeline(text)

# Update label map based on actual output
label_map = {
    "positive": "إيجابي",
    "negative": "سلبي",
    "neutral": "محايد"
}

print("Raw Output:", result)
print("Mapped Result:", label_map[result[0]['label']])

model_name = "aubmindlab/bert-base-arabertv2-twitter-sentiment"

from transformers import pipeline

model_name = "Muhannedbsh/arabert-sentiment-model-MuhannedSh"
sentiment = pipeline("sentiment-analysis", model=model_name, tokenizer=model_name)

text = "الخدمة سيئة للغاية"
result = sentiment(text)
print("Raw Output:", result)

label_map = {"LABEL_0": "سلبي", "LABEL_1": "إيجابي"}
print("Mapped Result:", label_map[result[0]['label']])

model_name = "Muhannedbsh/arabert-sentiment-model-MuhannedSh"

!pip install pyngrok
from pyngrok import ngrok

# فتح نفق (tunnel) جديد
public_url = ngrok.connect(8501)
print("رابط التطبيق:", public_url)

# تشغيل التطبيق
!streamlit run /content/app.py --server.port 8501 --server.headless true &

import joblib

# حفظ النموذج
joblib.dump(model, 'sentiment_model.pkl')

# حفظ vectorizer (مثلاً TF-IDF أو CountVectorizer)
joblib.dump(vectorizer, 'vectorizer.pkl')

import joblib

model = joblib.load('sentiment_model.pkl')
vectorizer = joblib.load('vectorizer.pkl')

# حفظ النموذج
model.save_pretrained('./my_model_dir')
tokenizer.save_pretrained('./my_model_dir')

from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained('./my_model_dir')
tokenizer = AutoTokenizer.from_pretrained('./my_model_dir')

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

arabic_sentiment_analysis_2021_kaust_path = kagglehub.competition_download('arabic-sentiment-analysis-2021-kaust')
asalhi_train_extend_by_test_path = kagglehub.dataset_download('asalhi/train-extend-by-test')
asalhi_train_files_path = kagglehub.dataset_download('asalhi/train-files')
asalhi_test_files_path = kagglehub.dataset_download('asalhi/test-files')

print('Data source import complete.')

"""**Arabic Sentiment Analysis 2021 @ KAUST**

**Author of this file:** Ali Salhi ( [http://asalhi.info/](http://) )

Main Idea: **Using MARBERT with Test Set Feedback & improved data cleaning process (appending stems as extra text)**

**Best Scores (Macro F-Score)**
* On Orginal training set : **0.79220** (Using **train_all.csv**)
* On Extended training set (by using test1 set) : **0.79624**  (best score achivied during competition: **0.79349**) , file: **train_all_ext.csv**
  
![](https://i.ibb.co/r3PKXsS/Screen-Shot-2021-06-08-at-3-38-10.png)  
  
  Note1: Extending mechanisim is explained later within this notebook (Check Appindex 1 at the end)
  
  Note2:
  The code runs best with best predictions on this colab url:
  https://colab.research.google.com/drive/1o2ozOTrQayqpykkkC4it-X9RH7DZEJPD?usp=sharing
  
  Kaggle gives slightly less F-Score than colab, proppley related to packages versions.
  
**BERT Model used** :

[https://huggingface.co/UBC-NLP/MARBERT](http://)

  
**Main Refrences:**
- Abdul-Mageed, M., Elmadany, A., & Nagoudi, E. M. B. (2020). ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic. arXiv preprint arXiv:2101.01785.

- Antoun, W., Baly, F., & Hajj, H. (2020). Arabert: Transformer-based model for arabic language understanding. arXiv preprint arXiv:2003.00104.

**Acknowledgement**

Thank you Wissam Antoun, Mohamed Al Salti & Fady Baly for the great code examples at
https://github.com/aub-mind/arabert

**Part Zero:**
GPU Or CPU ?
"""

import torch

# If there's a GPU available...
if torch.cuda.is_available():

    # Tell PyTorch to use the GPU.
    device = torch.device("cuda")

    print('There are %d GPU(s) available.' % torch.cuda.device_count())

    print('We will use the GPU:', torch.cuda.get_device_name(0))
    !nvidia-smi

# If not...
else:
    print('No GPU available, using the CPU instead.')
    device = torch.device("cpu")

"""**Part One:**
Installing needed packages
"""

!pip install pyarabic
!pip install emoji
!pip install pystemmer
!pip install optuna==2.3.0
!pip install transformers==4.2.1

"""**Part Two:** Import needed packages"""

import numpy as np
import pandas as pd
import pyarabic.araby as ar

import re , emoji, Stemmer, functools, operator, string
import torch , optuna, gc, random, os

from tqdm import tqdm_notebook as tqdm
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score
from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer
from transformers.data.processors import SingleSentenceClassificationProcessor
from transformers import Trainer , TrainingArguments
from transformers.trainer_utils import EvaluationStrategy
from transformers.data.processors.utils import InputFeatures
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
from sklearn.utils import resample

import logging

logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)

"""**Part 4:** Define cleaning method"""

st =  Stemmer.Stemmer('arabic')
def data_cleaning (text):
  text = re.sub(r'^https?:\/\/.*[\r\n]*', '', text, flags=re.MULTILINE)
  text = re.sub(r'^http?:\/\/.*[\r\n]*', '', text, flags=re.MULTILINE)
  text = re.sub(r"http\S+", "", text)
  text = re.sub(r"https\S+", "", text)
  text = re.sub(r'\s+', ' ', text)
  text = re.sub("(\s\d+)","",text)
  text = re.sub(r"$\d+\W+|\b\d+\b|\W+\d+$", "", text)
  text = re.sub("\d+", " ", text)
  text = ar.strip_tashkeel(text)
  text = ar.strip_tatweel(text)
  text = text.replace("#", " ");
  text = text.replace("@", " ");
  text = text.replace("_", " ");
  translator = str.maketrans('', '', string.punctuation)
  text = text.translate(translator)
  em = text
  em_split_emoji = emoji.get_emoji_regexp().split(em)
  em_split_whitespace = [substr.split() for substr in em_split_emoji]
  em_split = functools.reduce(operator.concat, em_split_whitespace)
  text = " ".join(em_split)
  text = re.sub(r'(.)\1+', r'\1', text)
  text_stem = " ".join([st.stemWord(i) for i in text.split()])
  text = text +" "+ text_stem
  text = text.replace("آ", "ا")
  text = text.replace("إ", "ا")
  text = text.replace("أ", "ا")
  text = text.replace("ؤ", "و")
  text = text.replace("ئ", "ي")

  return text

"""**Part 5:** Prepare Training Set"""

# Define the variables needed:

Use_Train_Extended_Data = True #set False if using orginal training set with not feedback from test1

Tweets_Ids_Col_Train ="Tweet_id"
Tweets_Text_Col_Train = "Text"
Tweets_Sentiment_Col_Train = "sentiment"
Train_Data_File = "../input/train-files/train_all.csv"
Train_Data_Extended_File = "../input/train-files/train_all_ext.csv"

train_data = pd.DataFrame()

if Use_Train_Extended_Data :
  train_data = pd.read_csv(Train_Data_Extended_File, sep=",")
else :
  train_data = pd.read_csv(Train_Data_File, sep=",")

print(train_data[Tweets_Sentiment_Col_Train].value_counts())
print(train_data.value_counts())

"""**Part 6:** Cleaning Traing Data"""

# Cleaning Training Data
train_data[Tweets_Text_Col_Train] = train_data[Tweets_Text_Col_Train].apply(lambda x:   data_cleaning(x))

# Removing un-needed feilds
if Tweets_Ids_Col_Train in train_data.columns:
  del train_data[Tweets_Ids_Col_Train]
train_data.columns = [Tweets_Sentiment_Col_Train,Tweets_Text_Col_Train]

train_data[Tweets_Text_Col_Train].head(50)

"""**Part 7:** Spliting Training Data (Train , Evaluation)"""

# First setting the max_len , will be useful later for BERT Model
Extra_Len = 6 # an extra padding in length , found to be useful for increasing F-score
Max_Len = train_data[Tweets_Text_Col_Train].str.split().str.len().max() + Extra_Len
print(Max_Len)

#Spliting the Training data
Test_Size = 0
if Use_Train_Extended_Data :
  Test_Size = 0.001  # low percentage to keep the training data as large as possible,
                     # the value 0.001 found to be best for F-Score with extended data
else :
  Test_Size = 0.0005 # low percentage to keep the training data as large as possible,
                     # the value 0.0005 found to be best for F-Score with original data
Rand_Seed = 42

train_set, evaluation_set = train_test_split( train_data, test_size= Test_Size, random_state= Rand_Seed)

print("Train set: ")
print(train_set[Tweets_Sentiment_Col_Train].value_counts())
print("---------------------------")
print ("Evaluation set: ")
print (evaluation_set[Tweets_Sentiment_Col_Train].value_counts())

"""**Part 8:** Preparing Test Data"""

# preparing test_data (which will be sumbitted to kaggle)

Tweets_Ids_Col_Test = "Tweet_id"
Tweets_Text_Col_Test = "Text"
Test_Data_File = "../input/test-files/test1_with_text.csv"

test_data = pd.read_csv(Test_Data_File, sep=",")
test_data.columns = [Tweets_Ids_Col_Test,Tweets_Text_Col_Test]

test_data[Tweets_Text_Col_Test] = test_data[Tweets_Text_Col_Test].apply(lambda x:   data_cleaning(x))
test_data[Tweets_Text_Col_Test].head(50)

"""**Part 9:** Preparing BERTModel Classes"""

Model_Used = "UBC-NLP/MARBERT"
Task_Name = "classification"

class Dataset:
    def __init__(
        self,
        name,
        train,
        test,
        label_list,
    ):
        self.name = name
        self.train = train
        self.test = test
        self.label_list = label_list

class BERTModelDataset(Dataset):
    def __init__(self, text, target, model_name, max_len, label_map):
      super(BERTModelDataset).__init__()
      self.text = text
      self.target = target
      self.tokenizer_name = model_name
      self.tokenizer = AutoTokenizer.from_pretrained(model_name)
      self.max_len = max_len
      self.label_map = label_map

    def __len__(self):
      return len(self.text)

    def __getitem__(self,item):
      text = str(self.text[item])
      text = " ".join(text.split())

      encoded_review = self.tokenizer.encode_plus(
      text,
      max_length= self.max_len,
      add_special_tokens= True,
      return_token_type_ids=False,
      pad_to_max_length=True,
      truncation='longest_first',
      return_attention_mask=True,
      return_tensors='pt'
    )
      input_ids = encoded_review['input_ids'].to(device)
      attention_mask = encoded_review['attention_mask'].to(device)

      return InputFeatures(input_ids=input_ids.flatten(), attention_mask=attention_mask.flatten(), label=self.label_map[self.target[item]])

"""**Part 10:** Defining Needed Methods for training and evaluation"""

def model_init():
  return AutoModelForSequenceClassification.from_pretrained(Model_Used, return_dict=True, num_labels=len(label_map))

def compute_metrics(p): #p should be of type EvalPrediction
  preds = np.argmax(p.predictions, axis=1)
  assert len(preds) == len(p.label_ids)
  print(classification_report(p.label_ids,preds))
  #print(confusion_matrix(p.label_ids,preds))

  macro_f1_pos_neg = f1_score(p.label_ids,preds,average='macro',labels=[1,2])
  macro_f1 = f1_score(p.label_ids,preds,average='macro')
  macro_precision = precision_score(p.label_ids,preds,average='macro')
  macro_recall = recall_score(p.label_ids,preds,average='macro')
  acc = accuracy_score(p.label_ids,preds)
  return {
      'macro_f1' : macro_f1,
      'macro_f1_pos_neg' : macro_f1_pos_neg,
      'macro_precision': macro_precision,
      'macro_recall': macro_recall,
      'accuracy': acc
  }

def set_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    np.random.seed(seed)
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)

"""**Part 11:** Build Train and Evaluation Data Sets"""

label_list = list(train_set[Tweets_Sentiment_Col_Train].unique())

print(label_list)
print(train_set[Tweets_Sentiment_Col_Train].value_counts())

data_set = Dataset( "KAUST", train_set, evaluation_set, label_list )

label_map = { v:index for index, v in enumerate(label_list) }
print(label_map)

train_dataset = BERTModelDataset(train_set[Tweets_Text_Col_Train].to_list(),
                                 train_set[Tweets_Sentiment_Col_Train].to_list(),Model_Used,Max_Len,label_map)

evaluation_dataset = BERTModelDataset(evaluation_set[Tweets_Text_Col_Train].to_list(),
                                      evaluation_set[Tweets_Sentiment_Col_Train].to_list(),Model_Used,Max_Len,label_map)

"""**Part 12:** Definge Training Arguments"""

#define training arguments
training_args = TrainingArguments("./train")
training_args.lr_scheduler_type = 'cosine'
training_args.evaluate_during_training = True
training_args.adam_epsilon =1e-8
if Use_Train_Extended_Data :
    training_args.learning_rate = 1.215e-05 # use this with extended data
else:
    training_args.learning_rate = 1.78255000000000001e-05 # use this with org data
training_args.fp16 = True
training_args.per_device_train_batch_size = 16 #64
training_args.per_device_eval_batch_size = 16 # 64
training_args.gradient_accumulation_steps = 2
training_args.num_train_epochs= 2
training_args.warmup_steps = 0
training_args.evaluation_strategy = EvaluationStrategy.EPOCH
training_args.logging_steps = 200
training_args.save_steps = 100000
training_args.seed = 42
training_args.disable_tqdm = False

"""Part 13: Build Trainer"""

training_args.dataloader_pin_memory = False
gc.collect()
torch.cuda.empty_cache()
set_seed(Rand_Seed)

trainer = Trainer(
    model = model_init(),
    args = training_args,
    train_dataset = train_dataset,
    eval_dataset= evaluation_dataset,
    compute_metrics=compute_metrics
)

print(training_args.seed)

"""**Part 14:** Train !"""

print(Max_Len)
print(training_args.learning_rate)
print(training_args.adam_epsilon)
print(training_args.warmup_steps)
#wandbkey if needed (depend on the transformers package version) = 0a58b374c46a154de1ba77c8634c6be279a9dcdb
trainer.train()

"""**Part 15:** Lets Predict on test1 data !"""

# first define the predection method
def predict(text, tokenizer):

  encoded_review = tokenizer.encode_plus(
    text,
    max_length=Max_Len,
    add_special_tokens=True,
    return_token_type_ids=False,
    pad_to_max_length=True, #True,
    truncation='longest_first',
    return_attention_mask=True,
    return_tensors='pt'
  )

  input_ids = encoded_review['input_ids'].to(device) #(input_ids + ([tokenizer.pad_token_id] * padding_length)).to(device)
  attention_mask = encoded_review['attention_mask'].to(device)


  output = trainer.model(input_ids, attention_mask)
  _, prediction = torch.max(output[0], dim=1)
  return prediction[0]

#then lets play !

tokenizer = AutoTokenizer.from_pretrained(Model_Used)

prediction_list = []
i = 0
for tweet in test_data[Tweets_Text_Col_Test]:
    id = test_data[Tweets_Ids_Col_Test][i]

    pre = predict(tweet,tokenizer)
    pre_txt = label_list[pre]

    if pre_txt == 'positive': pre_txt = 1
    if pre_txt == 'negative': pre_txt = -1
    if pre_txt == 'neutral': pre_txt = 0
    prediction_list.append(pre_txt)

    i = i + 1

"""**Part 16:** Save the prediction"""

#print(prediction_list)
results = pd.DataFrame({'Tweet_id' : test_data[Tweets_Ids_Col_Test].astype(str), 'sentiment' : prediction_list},
                       columns = ['Tweet_id', 'sentiment'])
print(results)


os.chdir(r'/kaggle/working')
result_file = "sub_test3.csv"
results.to_csv(result_file, sep= ",", index = False)

from IPython.display import FileLink
FileLink(r'sub_test3.csv')

"""Colab version scores: 0.79624

Kaggle version scors : 0.79581 (sub_test3.csv)

**Appindex 1 : Creating the Extended Training dataset from test1**

The extended data set which pushes the score from 0.79220 to 0.79624 was generated as follow:

1. I selected X learning rates (X = 24 currently).
2. I fixed the batch size to 32 and test size to 0.1 on the orginal training data
3. Seed was 42 with warmup_steps to zero.
4. For X (X = 24) times I ran the above code and generated a submission file (sub_<num>.csv).
5. The files can be found on "train-extend-by-test" folder.
6. The following table shows the files and thier local results (not tested on test1) only evaluation.

![](https://i.ibb.co/F6G8HGS/Screen-Shot-2021-06-08-at-2-35-11.png)    
   
7. The following chart shows a range of the learing rates and thier results on test1
![](https://i.ibb.co/8YpP8vj/Screen-Shot-2021-06-08-at-3-22-53.png)   

8. The 24 files then were merged to filter the tweets with same result in all of them (for example if negative then the tweet will score -24 (-1 * 24 times) , if positive the tweet will score 24 (1 * 24) , if neutral the tweet will score 0 (0 * 24) , any other value means that there was a change in the result on one or more learning rates, and that tweet will be droped.
    
9. A total of 18614 tweets out of 20000 in test1 had either a score of 24 or -24 or 0, those tweets were appended to the orginal training set to generate the train_all_ext set.
    
10. Below is the code that was used to do so.
"""

# Do not run, unless you need to generate the train_all_ext.csv again.

import pandas as pd
import numpy as np
res = pd.read_csv('../input/train-extend-by-test/'+'sub_1.csv', sep =",", dtype={'Tweet_id': np.int64})
files_to_train = ['sub_2.csv', 'sub_3.csv', 'sub_4.csv', 'sub_5.csv', 'sub_6.csv',
                  'sub_7.csv', 'sub_8.csv', 'sub_9.csv', 'sub_10.csv', 'sub_11.csv',
                  'sub_12.csv', 'sub_13.csv', 'sub_14.csv', 'sub_15.csv', 'sub_16.csv',
                  'sub_17.csv', 'sub_18.csv', 'sub_19.csv', 'sub_20.csv','sub_21.csv',
                  'sub_22.csv', 'sub_23.csv','sub_24.csv']
for i in files_to_train:
  t = pd.read_csv('../input/train-extend-by-test/'+i, sep =",", dtype={'Tweet_id': np.int64})
  res = pd.merge(res, t, on='Tweet_id', how='inner')


res['Tweet_id'] = res['Tweet_id'] .astype(str)
res['total'] = res.sum(numeric_only=True , axis=1)
res.head()
zero = 0
positive_limit = len(files_to_train) + 1
negative_limit =  -1 * positive_limit

print(positive_limit)
print(negative_limit)

res = res.loc[(res['total'] == positive_limit) |
            (res['total'] == negative_limit) |
            (res['total'] == zero)]

test1_file = pd.read_csv('../input/test-files/test1_with_text.csv', sep =",", dtype={'Tweet_id': str})

res = pd.merge(res , test1_file, on='Tweet_id', how='inner')


res = res [['Tweet_id','total','Text']]
res['total'] = res['total'].map({positive_limit:'positive',
                              negative_limit: 'negative',
                              zero: 'neutral'})
res = res.rename(columns={'total': 'sentiment'})

train_org = pd.read_csv('../input/train-files/train_all.csv', sep =",", dtype={'Tweet_id': str})

print(len(res))
train_ext = train_org.append(res, ignore_index = True)
train_ext['sentiment'].value_counts()

# result_file = "train_all_ext.csv"
# res.to_csv(result_file, sep= ",", index = False)

os.chdir(r'/kaggle/working')
result_file = "train_all_ext_demo.csv"
results.to_csv(result_file, sep= ",", index = False)

print(results.count())

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

# import numpy as np # linear algebra
# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# # Input data files are available in the read-only "../input/" directory
# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

# import os
# for dirname, _, filenames in os.walk('/kaggle/input'):
#     for filename in filenames:
#         print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session